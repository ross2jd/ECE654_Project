\documentclass[10pt,oneside]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{courier}
\usepackage{color,soul}
\usepackage{algpseudocode}
\usepackage[ruled]{algorithm}
\usepackage{amssymb}

\usepackage[
backend=biber,
style=numeric,
sorting=ynt,
citestyle=numeric
]{biblatex}
\addbibresource{ref.bib}



\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}

\title{Software Reliability Report}
\author{Jordan A. Ross \\j25ross@uwaterloo.ca}
\date{July 3rd, 2015}

\begin{document}

\maketitle
\newpage

\section{Introduction}
In this paper we provide an overview for the software reliability course project in which static analysis
is incorporated in the Clafer tool chain to aid users in modeling. Static analysis has become a popular
tool in recent years for finding bugs and discouraging bad programming practices (\hl{Reference needed}).
The attractiveness of static analysis is that recent research has shown the effectiveness of tools
such as FindBugs which aim to catch simple programming errors (\hl{Reference needed}). At the same time, FindBugs
and other tools such as Coverity also has sophisticated analysis to catch more serious errors in programs.

This project and paper aims to first understand the difference between tools such as FindBugs and Coverity
and in which aspects they excel. Secondly, I attempt to apply these static analysis techniques to Clafer, a
lightweight modeling language. The goal is to show that these techniques can be applied to modeling languages
to aid users that may make simple mistakes as they learn or help more experienced users from committing serious
errors.

The paper is structured as follows: it begins with an overview of the Clafer lanaguage and its current use
cases. Section 3 then gives an overview and a comparision of existing static analysis techniques with a focus
on the driving forces behind Coverity and FindBugs. Section 4 proposes how theses techniques could be applied
to Clafer and the possible benifits in doing so. Section 5 then goes into the implementation details of how
we incorparted this analysis into the Clafer tool chain. Section 6 gives some results of our implemention on
various models and discussion. The paper is then concluded in Section 7.

\section{Clafer Overview}
Clafer (\underline{cla}ss, \underline{fe}ature, \underline{r}eference) is a lightweight modeling language.
The original driving force behind the development of Clafer was feature modeling and product-line modeling.
Now, Clafer is used to model a wide range of doamins as it is not a domain specific language like EAST-ADL
or AADL (\hl{Reference needed}). The language of Clafer has minimialistic syntax and has the support of
formal semantics (\hl{Reference needed}).
% Maybe explain how it builds on Alloy?

One of the key features of Clafer is the rich support for modeling variability. The following subsections
will use examples to help describe some of the Clafer constructs that are of importance in this paper.
For a more formal semantic defintion of the language and complete overview, see  (\hl{Reference needed}).
Throughout this paper the running example of a automotive architecture will be used.

\subsection{Concrete and Abstract}
In Clafer everything is a \textit{Clafer}\footnote{We use the italic \textit{Clafer} to denote the construct
and the plain Clafer to denote the language} and there are two types, abstract and concrete. An abstract
\textit{Clafer} can be thought of as a type (or a class in object oriented programming). A concrete
\textit{Clafer} represents a concrete element in the model. These concrete elements will result in an
instance of the model. Concrete \textit{Clafers} can be typed using abstract ones (see the next section
for more details). Listing \ref{lst:concreteAbstract} shows a concrete and abstract example in the
context of a software component.

\begin{lstlisting}[label={lst:concreteAbstract},caption={Example of abstract and concrete \textit{Clafers}}]
abstract SoftwareComponent
mainComponent
\end{lstlisting}

\subsection{Inheritance}
Similiar to that of UML, Clafer provides first class support for inheritance. In the previous
example we had a \lstinline$mainComponent$ which should be a software component. Thus we can
\textit{type} it using inheritance. Listing \ref{lst:inheritanceEx} shows how this can be done.

\begin{lstlisting}[label={lst:inheritanceEx},caption={Inheritance example}]
abstract SoftwareComponent
mainComponent : SoftwareComponent
\end{lstlisting}

\subsection{Cardinality}
Clafer also has support for expressing cardinality of \textit{Clafers} which is similiar to
that of UML. Continuing the simple example, a new type \lstinline$Function$ can be introduced
and \textit{nested} under \lstinline$mainComponent$. Furthermore, a cardinality can be given to
\lstinline$function$ to express that there are 1 or more functions in the main component.
Listing~\ref{lst:cardinalityExample} shows the example expressing this.

\begin{lstlisting}[label={lst:cardinalityExample},caption={Example of nesting and cardinality}]
abstract SoftwareComponent
abstract Function

mainComponent : SoftwareComponent
  function : Function 1..*
\end{lstlisting}

\subsection{References}
References are a key feature to Clafer as they allow modelers to express relationships between
\textit{Clafers} that may not necessarily be nested. References are also overloaded to allow
for using integers in the language. In the previous section, cardinality and nesting were introduced
but the example is maybe not the best representation of a software component. For example, a modeler
may want every \lstinline$SoftwareComponent$ to be composed of 1 or more functions. Then in an instance
of \lstinline$mainComponent$ the modeler might want to specify which functions are in the main component.
This is done through the user of references and Listing~\ref{lst:referenceExample} shows how this would
be done in Clafer. In this listing a constraint is used which is explained in more detail in the following
section.

\begin{lstlisting}[label={lst:referenceExample},caption={Example use of references}]
abstract Function
abstract SoftwareComponent
  function -> Function 1..*

mainComponent : SoftwareComponent
  mainFunction : Function
  auxFunction : Function
  [function = (mainFunction, auxFunction)]
\end{lstlisting}

\subsection{Constraints}
Constraints come in different forms and are used throughout the language even though the modeler may be
unaware. For example, when a \textit{Clafer} is nested under another a constraint is imposed that every
instance of the parent \textit{Clafer} must have the child present. In the previous section, an explicit
constraint was used to assign what functions belonged to \lstinline$mainComponent$. Constraints can
take many forms and the reader should see (\hl{Reference needed}) for a full list of constraints
supported by the language.
\subsection{Backend Tool Support}
Clafer alone requires no tool support and much like UML can be written down on just pen and paper.
However, the tool support for Clafer allows a user to give a model written in Clafer and to generate
all valid instances of the model. Currently the Clafer tool chain supports two backends, Alloy 4.1
(\hl{Reference needed}) and Choco CSP (\hl{Reference needed}). The Choco backend also allows for
multi-objective optimization (\hl{Reference needed}).

\section{Static Analysis Techniques}
In this section of the report an overview of some of the state of the art static analysis techniques
are examined. Static analysis has been used heavily in programming over the years, one of the most
common analysis was the analysis of types in a program and ensuring that a program can be typed checked.
A program being type checked (or saying that a program is typeable) gives a guarentee that the program
will behave properly at runtime (\hl{Reference to FJ needed}).

In the past decade or so though a lot offocus and work has been done in trying to detect the presence of bugs that traditional type system may not catch (\hl{Reference needed}). This work has led to at least two successful and popular tools in industry, namely FindBugs and Coverity. Both tools are static analysis tools that perform some static analysis that does not necesarily provide a guarentee that the program will behave properly at runtime, but rather probable bugs that the developer(s) may have made. The following two subsections describe two of the prevelant techniques used in both Coverity and FindBugs and why they are effective. The last subsection describes both tools in more detail and comparison between the two.

%TODO: I am thinking that we should combine the specification sections and then have a section on the static analysis techniques such as dataflow analysis and controlflow.
\subsection{Specific Specifications}
In 2000 Engler et. al. proposed a methodology and implementation for \textit{meta-level compilation} (MC) \cite{5}. MC was proposed as the authors wanted to find a way to check properties in the code such as ``interrupts must be enabled after being disabled". Of course, testing can help find some of these bugs in production code but many tests are required for all of the exection paths of the code to test this property. Also, the authors point out that one could build a rigorous specification of the program and then use model checkers to verify \cite{5}. A rigorous specification is costly and usually difficult to create so many industies don't create them.

MC is a lower cost solution that takes as input some specifications or properties written by the developer (in some pattern matching language). In the paper the authors use \textit{metal} for their implementation \cite{5}. This approach thus doesn't require the developer (or project lead) to write a formal specification for the entire program but rather just a few critical properties such as interrupts and locks.

In \cite{5} they also test their implementation on large open source projects such as Linux and OpenBSD. In doing so they found that this technique at a local level was useful in finding certain bugs with low false postive rates.

% Talk about difference between local and global analysis.

Overall MC is rather useful and succesful static analysis technique for finding real bugs in well-tested and frequently used software. It is curious though that some of the properties that the authors wanted to check had very good false positive rates while others, such as checking mutex's was very poor \cite{5}.

Another approach of having specific specifications comes from the work of Hovemeyer and Pugh. In \cite{6} they propose to investigate simple static analysis techniques for finding bugs based on the notion of \textit{bug patterns}. These patterns are concrete specifications that should hold for the code that is analyzed. In the paper the authors highlight several of the patterns the check for with their tool FindBugs.

% Include some more about the FindBugs detectors and some of the simpler patterns they use.

Unlike \cite{5}, in this implementation they are running conducting the analysis and pattern matches on the byte code of Java whereas Engler et. al. were writing specifications for the \textit{g++} compiler and linking to it.
\subsection{Inferring Specifications}
In 2001 Engler built on his previous work in \cite{5} to propose another static analysis technique to help find more bugs in code. Engler et. al introduced the concept of \textit{beliefs} which is the notition of inferring what the programmer intended to do in the code \cite{7}. The authors propose two types of beliefs, MAY and MUST beliefs. MUST beliefs are much like some of the bug patterns and meta-level compilation. A concrete example would be that a pointer null dereference implies that the programmer must beleive the pointer is non-null \cite{7}.

A MUST belief only requires a single contradiction (i.e. the pattern must be present only once) in order for an error to be found. This is equivelant to saying that the pattern in FindBugs must exist just one and they will throw warning or error message.
% MUST beliefs and FindBugs should be placed together I think...

MAY beliefs are the case when it is suggested that an observed pattern was programmers intent where in fact it may be a coincidence \cite{7}. A concrete example of this would be that two methods must be called together, like \textit{``a" must be followed by a call to ``b"}. In \cite{7} the approach is to treat all MAY beliefs as MUST beliefs, thus record an error everything this belief is violated, then using statistical analysis the number of false positives (coincidences) are reduced.

The analysis performed in \cite{7} is different than that of MC in the fact that a template based approach is used to mine MAY beliefs. For example, we may not know what ``a" or ``b" is in the pattern but we want to observe they are called in the same order, or if they are simply called together.
% Key points to hit:
% May beliefs.
% Dataflow analysis
% AST analysis
% The statistical analysis to reduce false positives.

\subsection{FindBugs and Coverity}
% Key points to hit:
% See if we can find projects that they were both run on and provide some graphic
% Provide a table given criteria where maybe they may differs
% Reference the Coverity slides and the paper that compares the 2 for concurrency


\section{Static Analysis of Clafer}
% This section should come directly from the possible patterns markdown document.
In this section we present how static analysis could be incorporated into the Clafer tool chain and possible patterns to highlight the motivation for doing so. Since Clafer is a constraint based modeling language, there is no need for dataflow analysis. Instead, the abstract syntax tree (AST) can be traversed to look for the specified patterns. In the following four subsections each pattern is described with an example and some motivation for why the pattern is important to detect. %The last subsection describes what static analyis technique will be needed in order to warn users about these pattern violations.
%TODO: Should we have a section about what static analysis techniques we are going to use?

\subsection{Pattern 1: Missing Constraint from Inherited Clafer}
The first pattern is detecting when a user may have missed a constraint when creating a concrete instance of an abstract \textit{Clafer}. The following example in Listing~\ref{lst:pattern1Ex} shows a possible scenario where this might happen.
\begin{lstlisting}[label={lst:pattern1Ex},caption={Example of Pattern 1}]
abstract Location
abstract Edge
  location -> Location 2
  length ->> int

abstract HarnessEdge : Edge
  area ->> int

Harness
  l1 : Location
  l2 : Location
  l3 : Location

  e1 : HarnessEdge
    [length = 1]
    [area = 2]
  e2 : HarnessEdge
    [location = (l2, l3)]
    [length = 1]
    [area = 3]
\end{lstlisting}
The problem with this example is the \lstinline$e1$ can take any location that is defined which is a mistake. This could be a problem if the model is very large as it will exponentially increase the search space. What is possibly worse is the fact that when doing optimization it could give a solution that is not valid for our modeled system.

In order to catch this error the AST would need to be traversed for each concrete \textit{Clafer} that inherits for an abstract one. Then for each concrete the analysis would detect any missing constraints on on which child \textit{Clafer} of the inherited \textit{Clafer}. Once all violations are found (treating these MAY beliefs as MUST beliefs), statistical analyis would be performed to reduce the number of false positives. Section~\ref{sec:implementation} discusses the details of the implementation in more detail.

\subsection{Pattern 2: Missing ``.ref" of Clafer}
The second pattern deals with references between two \textit{Clafers}. In C++, to get the value of a reference you must dereference the pointer. Much like C++, in \textit{Clafer} in order to get the \textit{Clafer} that a reference is pointing to a ``.ref" must be used. One of the limitations of the Clafer type system is that there are cases where a constraint will type check but in fact it is not constraing what the user intended. The example in Listing~\ref{lst:pattern2Ex} shows a possible scenario where this could happen.
\begin{lstlisting}[label={lst:pattern2Ex},caption={Example of Pattern 2},numbers=left]
abstract DeviceNode
abstract FunctionalAnalysisComponent
    deployedTo -> DeviceNode
abstract Command
    sender -> FunctionalAnalysisComponent
    receiver -> FunctionalAnalysisComponent
    deployedTo -> LogicalDataConnector ?
        [parent in this.deployedFrom]
    [(sender.deployedTo = receiver.deployedTo) <=> no this.deployedTo]
\end{lstlisting}

In this example the user is trying to state ``The command should not be deployed to a logical data connector if and only if the sender and receiver are not deployed to the same device node". Line 9 will type check because both sides of the equals sign are references to DeviceNode. However, the user most likely intended to compare the concrete \textit{Clafers} that \lstinline$deployedTo$ referenced. Much like pattern 1, this is a major issue because the model will compile and the user will have no idea unless they check the instances generated. This again may be easy to do for smaller models, but as they grow this becomes unrealistic.

In order to catch this error the AST would need to be traversed looking for the pattern of when equality is expressed between two reference \textit{Clafers}. This analysis would always throw the error if this pattern is seen and the user would have the option to dismiss if this was intended.
%TODO: Rewrite this maybe once we have the implementation?
\subsection{Pattern 3: Missing implies restricts variability}
One of the key features of Clafer is it's ability to easily express variability in a model. However, Clafer has it's limitations and a user should be aware when the model is not modeling the amount of variability that the user intended. The example shown in Listing~\ref{lst:pattern3Ex} highlights a possible scenario.
\begin{lstlisting}[label={lst:pattern3Ex},caption={Example of Pattern 3}]
[fa.PinchDetectionFA.PositionSensor.deployedTo.ref = ht.dn.Motor]
\end{lstlisting}

The problem with this example is that \lstinline$PinchDetectionFA$ is an optional \textit{Clafer} that is constrained by some other \textit{Clafer} in the model. Now what is trying to be modeled is that a constraint needs to be applied to the nested \lstinline$PositionSensor$ \textbf{if} the \lstinline$PinchDetectionFA$ is present in the instance. Sometimes the solver will not be smart enough to handle this case so we need to put an implies constraint to be more explicity as follows:
\begin{lstlisting}[]
[fa.PinchDetectionFA => fa.PinchDetectionFA.PositionSensor.deployedTo.ref = ht.dn.Motor]
\end{lstlisting}

The reason for this pattern is quite obvious as the user may be totally unaware of the restrictions on the variability and thus the solver is returning possibly non-optimal solutions.

In order to catch this mistake a similiar approach to the Pattern 2 will be taken and each violoation will be treated as an error and the user will be warned about the possibility of restricted variability.

\subsection{Pattern 4: Incorrect use of ``=", should use ``in"}
The last pattern that is examined is one that will help aid users who are unfamiliar with the semantics of Clafer, more specificatly with the operators ``=" and ``in". Listing~\ref{lst:pattern4Ex} highlights a scenario that a user may make.
\begin{lstlisting}[label={lst:pattern4Ex},caption={Example of Pattern 4},numbers=left]
abstract FunctionalAnalysisComponent
  deployedTo -> DeviceNode
    [parent in this.deployedFrom]

abstract DeviceNode
  deployedFrom -> FunctionalAnalysisComponent *
    [this.deployedTo = parent]

function1 : FunctionalAnalysisComponent
  [deployedTo = (device1, device2)]

device1 : DeviceNode
device2 : DeviceNode
\end{lstlisting}

The example is incorrect because line 10 should be \lstinline$[deployedTo in (device1, device2)]$. The reason it should be this is that deployedTo should only be in the set of device1 and device2. What is currently written in the model is that deployedTo should contain both elements of the set device1 and device2 which is unsatisfiable. Therefore the solver will return 0 solutions and the user will have to manually track down the error.

Much like the previous two, this pattern would use the AST to find the corresponding pattern and always throw a warning when it finds a match.

\section{Implementation}
\label{sec:implementation}
In this section the implementation of the patterns described in the previous section is described. For each pattern the algorithm is given for how the pattern was analyzed. Along with the algorithm, each pattern describes how warnings are issued to the user (i.e. if any statistical analysis or filtering is applied to the raw errors).

% We should talk about how we make use of the clafer choco AST
\subsection{Pattern 1}
Recall that pattern 1 was to find instances in the model where a modeler may have mistakenly forgetten to constrain a \textit{Clafer} from its super \textit{Clafer}. The algorithm for this pattern is given in Algorithm~\ref{alg:pattern1}.

\begin{algorithm}[H]
\caption{Unconstrained Sub Clafers}\label{alg:pattern1}
\begin{algorithmic}[1]
\Procedure{analyzeUnconstrainedRefClafers}{}
  \State \textbf{let} $X$ be an order set of abstract Clafers, sub before super Clafers
  \ForAll{$X_i \in Reverse(X)$}\Comment{Loop through $X$ in reverse order}
    \If{$X_i$ has a super Clafer $X_j$}
      \State $X_i.set \gets$ \Call{findUnconstrainedRefClafersOfAbstract}{$X_i$, $X_j.set$}
    \Else
      \State $X_i.set \gets$ \Call{findUnconstrainedRefClafersOfAbstract}{$X_i$, $\emptyset$}
    \EndIf
  \EndFor
  \ForAll{$X_i \in X$}
    \State \textbf{let} $Z$ be the set of concrete Clafers that inherit from $X_i$
    \ForAll{$s \in X_i.set$}
      \ForAll{$Z_i \in Z$}
        \If{$\exists constraint(s) \in Z_i$}
          \State Log success
        \Else
          \State Log error
        \EndIf
      \EndFor
      \State Analyze warnings for $X_i$
    \EndFor
  \EndFor
\EndProcedure
\Procedure{findUnconstrainedRefClafersOfAbstract}{$a,S$}\Comment{The unconstrained Clafers of a given a propogated set of unconstrained Clafers S}
   \ForAll{$s \in S$}
    \If{$\exists constraint(s) \in a$}
      \State $S \gets S \setminus \{s\}$
    \EndIf
   \EndFor
   \ForAll{$child \in childrenWithRef(a)$}
    \If{$\nexists constraint(child) \in a$}
      \State $S \gets S \cup \{child\}$
    \EndIf
   \EndFor
   \State \Return $S$
\EndProcedure
\end{algorithmic}
\end{algorithm}

The algorithm will analyze each of the abstract \textit{Clafers} first so that that if another abstract Clafer inherits from another and constrains its super \textit{Clafers} a false positive is not thrown. The algorithm does not show how any statistical analysis that is done on the warnings log for each abstract \textit{Clafer}. In order to reduce false positives some analysis needs to be done on the logged warning messages. Algorithm~\ref{alg:pattern1Stats} describes the process of how an warning is generated based on some simple analysis.

\begin{algorithm}[H]
\caption{Analysis Procedure on Warnings for Unconstrained Reference Clafers}\label{alg:pattern1Stats}
\begin{algorithmic}[1]
\Procedure{warningAnalysis}{X}
  \State \textbf{let} $a$ be the context abstract Clafer
  \State \textbf{let} $V$ be the set of concrete Clafers that have an error logged.
  \State $successCnt \gets $ total successes for $a$
  \State $errorCnt \gets $ total errors for $a$
  \If{$successCnt != 0$}
    \State Generate warnings $\forall v \in V$
  \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

In the analysis we don't thow any warnings if there are no successes since its obvious the user didn't want to constrain the Clafer. More sophisticated statistical analysis is not used since the model would like to know if there are any unconstrained reference Clafers. It should be up to the modeler to determine if it was they inteded or not. In future interations of this tool it would be good if the modeler could mark warnings as intended so they are not shown up on future executions of the tool. This reasoning is valid in the context of modeling as models are not nearly as large as the industrial size code bases so a modeler could more easily traverse through the warnings to determine what was intended or not.

\subsection{Pattern 2}
In pattern 2 the goal is to notify the modler when they may be missing the ``.ref" on  \textit{Clafers} inside of the a constraint. Algorithm~\ref{alg:pattern2} describes how the analysis is carried out.

\begin{algorithm}[H]
\caption{Finding missing ``.ref" of Clafer in Constraints}\label{alg:pattern2}
\begin{algorithmic}[1]
\Procedure{findingMissingRefOfClafers}{}
  \State \textbf{let} $C$ be the set of constraints with equality
  \ForAll{$c \in C$}
    \State $left \gets c.getLeftOfEqual()$
    \State $right \gets c.getRightOfEqual()$
    \If{$left.hasRef() \&\& right.hasRef()$}
      \If{$left || right$ does not a trailing $.ref$}
        \State log error
      \EndIf
    \EndIf
  \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Results \& Discussion}

\medskip

\printbibliography

\end{document}
